{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ad853e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import defaultdict\n",
    "\n",
    "def load_dataset(filepath):\n",
    "    \"\"\"\n",
    "    Load transactions from a file where each line is a transaction with item IDs separated by spaces.\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r', encoding='ISO-8859-1') as file:  # Specify encoding\n",
    "        transactions = [line.strip().split() for line in file]\n",
    "    return transactions\n",
    "\n",
    "def get_support_counts(transactions, itemsets):\n",
    "    \"\"\"\n",
    "    Calculate the support count for each itemset in the list of transactions.\n",
    "    \"\"\"\n",
    "    support_counts = defaultdict(int)\n",
    "    for transaction in transactions:\n",
    "        transaction_set = set(transaction)\n",
    "        for itemset in itemsets:\n",
    "            if itemset.issubset(transaction_set):\n",
    "                support_counts[itemset] += 1\n",
    "    return support_counts\n",
    "\n",
    "def generate_candidates(frequent_itemsets, k):\n",
    "    \"\"\"\n",
    "    Generate candidate itemsets of size k based on frequent itemsets of size k-1.\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    frequent_items = list(frequent_itemsets.keys())\n",
    "    for i in range(len(frequent_items)):\n",
    "        for j in range(i + 1, len(frequent_items)):\n",
    "            candidate = frequent_items[i].union(frequent_items[j])\n",
    "            if len(candidate) == k:\n",
    "                candidates.append(candidate)\n",
    "    return set(candidates)\n",
    "\n",
    "def apriori(transactions, min_support_count):\n",
    "    \"\"\"\n",
    "    Run the Apriori algorithm on transactions with a specified minimum support count.\n",
    "    \"\"\"\n",
    "    single_items = defaultdict(int)\n",
    "    for transaction in transactions:\n",
    "        for item in transaction:\n",
    "            single_items[frozenset([item])] += 1\n",
    "    frequent_itemsets = {itemset: count for itemset, count in single_items.items() if count >= min_support_count}\n",
    "\n",
    "    k = 2\n",
    "    all_frequent_itemsets = frequent_itemsets.copy()\n",
    "    while frequent_itemsets:\n",
    "        candidates = generate_candidates(frequent_itemsets, k)\n",
    "        candidate_support_counts = get_support_counts(transactions, candidates)\n",
    "        frequent_itemsets = {itemset: count for itemset, count in candidate_support_counts.items() if count >= min_support_count}\n",
    "        all_frequent_itemsets.update(frequent_itemsets)\n",
    "        k += 1\n",
    "\n",
    "    return all_frequent_itemsets\n",
    "\n",
    "def find_frequent_patterns(filepath, min_support_counts):\n",
    "    \"\"\"\n",
    "    Run the Apriori algorithm for each minimum support count in min_support_counts and return results.\n",
    "    \"\"\"\n",
    "    transactions = load_dataset(filepath)\n",
    "    results = {}\n",
    "    for min_support in min_support_counts:\n",
    "        print(f\"Running Apriori with minimum support count = {min_support}\")\n",
    "        frequent_itemsets = apriori(transactions, min_support)\n",
    "        results[min_support] = frequent_itemsets\n",
    "        print(f\"Found {len(frequent_itemsets)} frequent itemsets with min support {min_support}\")\n",
    "    return results\n",
    "\n",
    "filepath = '/content/drive/MyDrive/Colab Notebooks/DATASETS/retail.txt'\n",
    "min_support_counts = [100, 500, 1000, 1500]\n",
    "\n",
    "results = find_frequent_patterns(filepath, min_support_counts)\n",
    "\n",
    "for min_support, itemsets in results.items():\n",
    "    print(f\"\\nMinimum Support: {min_support}\")\n",
    "    for itemset, count in itemsets.items():\n",
    "        print(f\"Itemset: {set(itemset)}, Support Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84868c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import defaultdict\n",
    "\n",
    "def load_dataset(filepath):\n",
    "    with open(filepath, 'r', encoding='ISO-8859-1') as file:  # Specify encoding to avoid Unicode errors\n",
    "        transactions = [line.strip().split() for line in file]\n",
    "    return transactions\n",
    "\n",
    "def get_support_counts(transactions, itemsets):\n",
    "    support_counts = defaultdict(int)\n",
    "    for transaction in transactions:\n",
    "        transaction_set = set(transaction)\n",
    "        for itemset in itemsets:\n",
    "            if itemset.issubset(transaction_set):\n",
    "                support_counts[itemset] += 1\n",
    "    return support_counts\n",
    "\n",
    "def generate_candidates(frequent_itemsets, k):\n",
    "    candidates = []\n",
    "    frequent_items = list(frequent_itemsets.keys())\n",
    "    for i in range(len(frequent_items)):\n",
    "        for j in range(i + 1, len(frequent_items)):\n",
    "            candidate = frequent_items[i].union(frequent_items[j])\n",
    "            if len(candidate) == k:\n",
    "                candidates.append(candidate)\n",
    "    return set(candidates)\n",
    "\n",
    "def apriori(transactions, min_support_count):\n",
    "    single_items = defaultdict(int)\n",
    "    for transaction in transactions:\n",
    "        for item in transaction:\n",
    "            single_items[frozenset([item])] += 1\n",
    "    frequent_itemsets = {itemset: count for itemset, count in single_items.items() if count >= min_support_count}\n",
    "\n",
    "    k = 2\n",
    "    all_frequent_itemsets = frequent_itemsets.copy()\n",
    "    while frequent_itemsets:\n",
    "        candidates = generate_candidates(frequent_itemsets, k)\n",
    "        candidate_support_counts = get_support_counts(transactions, candidates)\n",
    "        frequent_itemsets = {itemset: count for itemset, count in candidate_support_counts.items() if count >= min_support_count}\n",
    "        all_frequent_itemsets.update(frequent_itemsets)\n",
    "        k += 1\n",
    "\n",
    "    return all_frequent_itemsets\n",
    "\n",
    "def generate_association_rules(frequent_itemsets, min_confidence, transactions):\n",
    "    rules = []\n",
    "    transaction_count = len(transactions)\n",
    "\n",
    "    support_count = {itemset: count for itemset, count in frequent_itemsets.items()}\n",
    "\n",
    "    for itemset in frequent_itemsets.keys():\n",
    "        if len(itemset) < 2:\n",
    "            continue\n",
    "        for i in range(1, len(itemset)):\n",
    "            for lhs in itertools.combinations(itemset, i):\n",
    "                lhs = frozenset(lhs)\n",
    "                rhs = itemset - lhs\n",
    "                if rhs:\n",
    "                    confidence = support_count[itemset] / support_count[lhs]\n",
    "                    if confidence >= min_confidence:\n",
    "                        rule = (lhs, rhs, confidence)\n",
    "                        rules.append(rule)\n",
    "    return rules\n",
    "\n",
    "def find_association_rules(filepath, min_support_count, min_confidence_values):\n",
    "    transactions = load_dataset(filepath)\n",
    "    frequent_itemsets = apriori(transactions, min_support_count)\n",
    "\n",
    "    results = {}\n",
    "    for min_confidence in min_confidence_values:\n",
    "        print(f\"Generating rules with minimum confidence = {min_confidence}\")\n",
    "        rules = generate_association_rules(frequent_itemsets, min_confidence, transactions)\n",
    "        results[min_confidence] = rules\n",
    "        print(f\"Found {len(rules)} rules with min confidence {min_confidence}\")\n",
    "    return results\n",
    "\n",
    "filepath = '/content/drive/MyDrive/Colab Notebooks/DATASETS/retail.txt'  \n",
    "min_support_count = 1000  \n",
    "min_confidence_values = [0.5, 0.6, 0.7]  \n",
    "\n",
    "association_rules_results = find_association_rules(filepath, min_support_count, min_confidence_values)\n",
    "\n",
    "for min_confidence, rules in association_rules_results.items():\n",
    "    print(f\"\\nMinimum Confidence: {min_confidence}\")\n",
    "    for lhs, rhs, confidence in rules:\n",
    "        print(f\"Rule: {set(lhs)} => {set(rhs)}, Confidence: {confidence:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3442b68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_dataset(filepath):\n",
    "    with open(filepath, 'r', encoding='ISO-8859-1') as file:  # Specify encoding to avoid Unicode errors\n",
    "        transactions = [line.strip().split() for line in file]\n",
    "    return transactions\n",
    "\n",
    "def get_support_counts(transactions, itemsets):\n",
    "    support_counts = defaultdict(int)\n",
    "    for transaction in transactions:\n",
    "        transaction_set = set(transaction)\n",
    "        for itemset in itemsets:\n",
    "            if itemset.issubset(transaction_set):\n",
    "                support_counts[itemset] += 1\n",
    "    return support_counts\n",
    "\n",
    "def generate_candidates(frequent_itemsets, k):\n",
    "    candidates = []\n",
    "    frequent_items = list(frequent_itemsets.keys())\n",
    "    for i in range(len(frequent_items)):\n",
    "        for j in range(i + 1, len(frequent_items)):\n",
    "            candidate = frequent_items[i].union(frequent_items[j])\n",
    "            if len(candidate) == k:\n",
    "                candidates.append(candidate)\n",
    "    return set(candidates)\n",
    "\n",
    "def apriori(transactions, min_support_count):\n",
    "    single_items = defaultdict(int)\n",
    "    for transaction in transactions:\n",
    "        for item in transaction:\n",
    "            single_items[frozenset([item])] += 1\n",
    "    frequent_itemsets = {itemset: count for itemset, count in single_items.items() if count >= min_support_count}\n",
    "\n",
    "    k = 2\n",
    "    all_frequent_itemsets = frequent_itemsets.copy()\n",
    "    while frequent_itemsets:\n",
    "        candidates = generate_candidates(frequent_itemsets, k)\n",
    "        candidate_support_counts = get_support_counts(transactions, candidates)\n",
    "        frequent_itemsets = {itemset: count for itemset, count in candidate_support_counts.items() if count >= min_support_count}\n",
    "        all_frequent_itemsets.update(frequent_itemsets)\n",
    "        k += 1\n",
    "\n",
    "    return all_frequent_itemsets\n",
    "\n",
    "def generate_association_rules(frequent_itemsets, min_confidence, transactions):\n",
    "    rules = []\n",
    "    transaction_count = len(transactions)\n",
    "\n",
    "    support_count = {itemset: count for itemset, count in frequent_itemsets.items()}\n",
    "\n",
    "    for itemset in frequent_itemsets.keys():\n",
    "        if len(itemset) < 2:\n",
    "            continue\n",
    "        for i in range(1, len(itemset)):\n",
    "            for lhs in itertools.combinations(itemset, i):\n",
    "                lhs = frozenset(lhs)\n",
    "                rhs = itemset - lhs\n",
    "                if rhs:\n",
    "                    confidence = support_count[itemset] / support_count[lhs]\n",
    "                    if confidence >= min_confidence:\n",
    "                        rule = (lhs, rhs, confidence)\n",
    "                        rules.append(rule)\n",
    "    return rules\n",
    "\n",
    "def experiment_with_support_and_confidence(filepath, min_support_counts, min_confidence_values):\n",
    "    transactions = load_dataset(filepath)\n",
    "\n",
    "    support_pattern_counts = []\n",
    "    confidence_rule_counts = {min_conf: [] for min_conf in min_confidence_values}\n",
    "\n",
    "    for min_support in min_support_counts:\n",
    "        frequent_itemsets = apriori(transactions, min_support)\n",
    "        support_pattern_counts.append(len(frequent_itemsets))\n",
    "\n",
    "        for min_confidence in min_confidence_values:\n",
    "            rules = generate_association_rules(frequent_itemsets, min_confidence, transactions)\n",
    "            confidence_rule_counts[min_confidence].append(len(rules))\n",
    "\n",
    "    return support_pattern_counts, confidence_rule_counts\n",
    "\n",
    "filepath = '/content/drive/MyDrive/Colab Notebooks/DATASETS/retail.txt'  \n",
    "min_support_counts = [500, 1000, 1500, 2000]  \n",
    "min_confidence_values = [0.4, 0.5, 0.6, 0.7]  \n",
    "\n",
    "support_pattern_counts, confidence_rule_counts = experiment_with_support_and_confidence(filepath, min_support_counts, min_confidence_values)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(min_support_counts, support_pattern_counts, marker='o', color='b', label='Frequent Patterns')\n",
    "plt.xlabel('Minimum Support Count')\n",
    "plt.ylabel('Number of Frequent Patterns')\n",
    "plt.title('Frequent Patterns vs Minimum Support Count')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for min_confidence, rule_counts in confidence_rule_counts.items():\n",
    "    plt.plot(min_support_counts, rule_counts, marker='o', label=f'Min Confidence = {min_confidence}')\n",
    "plt.xlabel('Minimum Support Count')\n",
    "plt.ylabel('Number of Association Rules')\n",
    "plt.title('Association Rules vs Minimum Support Count')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
